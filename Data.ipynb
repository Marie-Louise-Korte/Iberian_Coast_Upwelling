{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9995496-b6fb-4eba-9afe-9ff577a82a54",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing the Data\n",
    "\n",
    "## ERA5 (Ekman Upwelling Index)\n",
    "I am working with the mean turbulent surface stress $\\big[\\frac{N}{m^2}\\big]$ or [Pa] in eastward and northward direction, available from the [Copernicus Climate Data Store](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview). I download the data with the Copernicus tool that resamples it from hourly to daily resolution. [Download Link](https://cds.climate.copernicus.eu/apps/user-apps/app-c3s-daily-era5-statistics?dataset=reanalysis-era5-single-levels&product_type=reanalysis&variable_e5sl=eastward_turbulent_surface_stress&pressure_level_e5sl=-&statistic=daily_mean&year_e5sl=2020&month=01&frequency=1-hourly&time_zone=UTC%2B00:00&grid_e5=0.25/0.25&area.lat:record:list:float=36&area.lat:record:list:float=45&area.lon:record:list:float=-20&area.lon:record:list:float=-5).\n",
    "- Period: 01/12/1981-31/01/2024 (daily)\n",
    "- Resolution: 0.25° x 0.25°\n",
    "\n",
    "### Processing\n",
    "1. Download data with ERA5_download.py\n",
    "    - conda activate IbUpPy3.9.12\n",
    "    - python3 ERA5_donwload_surface_stress.py\n",
    "2. Load the data\n",
    "    - after the download the data are all stored in individual files, I have one northward and eastward file per year\n",
    "    - load and combine the datasets\n",
    "    - save as MTSS.nc\n",
    "3. Add land mask\n",
    "    - get ERA5 land sea mask: Download_ERA5_land_sea_mask.ipynb\n",
    "    - add to MTSS\n",
    "4. Resample the data\n",
    "    - the data has daily resolution resample to weekly resolution (match the format of UI SST data as it already is at weekly resolution)\n",
    "    - wanted format: weekly mean Sat-Fr & time stamp Tue\n",
    "    - save the resampled data as MTSS_weekly.nc\n",
    "    - also calculate std and save as MTSS_weekly_std.nc\n",
    "5. (Ekaman transport -› calculate when needed)\n",
    "    - calculate Ekman transport from the wind stress data\n",
    "    - calculate the Ekman upwelling index (aka the westward component of the Ekman transport)\n",
    "    - (save as UI_Ek.nc -› don't)\n",
    "\n",
    "## SST Upwelling Index\n",
    "I downloaded the data from [CoastNET geoportal](http://geoportal.coastnet.pt). This product is calculated with SST data obtained from [CORTAD](ahttps://www.ncei.noaa.gov/products/coral-reef-temperature-anomaly-database)\n",
    "- Period: 04/01/1982 - 09/11/2021 (weekly)\n",
    "- Resolution: lat: ~0.04166° and lon: 5.019 - 0.04166° (lon res does not really matter because this index calculates the difference between the temperature on the mid-shelf and at 15°W)\n",
    "\n",
    "### Processing\n",
    "1. Download the data and save as UI_SST_CoastNET.nc\n",
    "2. Load the data\n",
    "3. Convert the temperature from Kelvin to °C for more intuitive understanding\n",
    "4. Change the sign of the index\n",
    "    - the index is calculated by substracting the midshelf temperatures from the 15°W temperatures ($T_{mid-shelf} \\ - \\ T_{15^{\\circ}W} \\ = \\ UI_{SST}$)\n",
    "    - multiply by -1 so that positive values indicate upwelling\n",
    "    - save as UI_SST.nc\n",
    "\n",
    "## ECCO2 \n",
    "### SST and SSH\n",
    "Download the [ECCO2 data](https://ecco.jpl.nasa.gov/drive/files/ECCO2/cube92_latlon_quart_90S90N/)\n",
    "- Period 01/01/1992 - 31/12/2023\n",
    "- Resolution:\n",
    "\n",
    "1. Download the data with Download_ECCO2.txt\n",
    "    - Download_ECCO2.txt is and executable file\n",
    "    - execute the file in the terminal by just running its name ./Download_ECCO2.txt\n",
    "3. Load the data\n",
    "4. Select research area (35°N to 45°N, 20°W (340°E) to 5°W (355°E))\n",
    "5. Resample the data\n",
    "    - the data has daily resolution resample to weekly resolution (match the format of UI SST data as it already is at weekly resolution)\n",
    "    - wanted format: weekly mean Sat-Fr & time stamp Tue\n",
    "    - save the resampled data as ECCO2_weekly.nc\n",
    "    - also calculate std and save as ECCO2_weekly_std.nc\n",
    "      \n",
    "## IBI SSH\n",
    "Downloaded the data for the Iberian Peninsula from [Copernicus Marine Serivce](https://data.marine.copernicus.eu/product/IBI_MULTIYEAR_PHY_005_002/description)\n",
    "- Period 01/01/1993 - 28/12/2021\n",
    "- Resolution: 0.083° x 0.083°\n",
    "\n",
    "### Processing\n",
    "1. Download the data with Download_SSH.ipynb\n",
    "    - rename latitude, lat and longitude, lon\n",
    "2. Load the data\n",
    "3. Rename lat, lon, time\n",
    "4. Cut research area from global dataset\n",
    "5. Resample the data\n",
    "    - the data has daily resolution resample to weekly resolution (match the format of UI SST data as it already is at weekly resolution)\n",
    "    - wanted format: weekly mean Sat-Fr & time stamp Tue\n",
    "    - save the resampled data as SSH_weekly.nc\n",
    "    - also calculate std and save as SSH_weekly_std.nc\n",
    "  \n",
    "## CoRTAD SST\n",
    "Download data from [NOAA](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.nodc:NCEI-CoRTADv6)\n",
    "- Period 01/05/1982 - 27/12/2022\n",
    "- Resolution: 0.04165649° x 0.04165649°\n",
    "\n",
    "### Processing\n",
    "1. Download the cortadv6_FilledSST.nc via the link on the HTTPS download on the NOAA website and save as CoRTAD_global.nc\n",
    "2. Drop the dimension 'nv'\n",
    "3. Select research area (35°N to 45°N, 20°W (340°E) to 5°W (355°E))\n",
    "4. Is the dataset to which we are all matching our weekly resampling (so don't need to do anything)\n",
    "5. Change °K to °C\n",
    "    - Save as CoRTAD_weekly.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7f1b831-f253-4575-a861-082821adb03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import packages\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import my_functions\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # ignore runtime warning for SSH.resample(...).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e35a0a10-66d3-4d43-8f21-d260916e462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set directory to where datasets are stored\n",
    "os.chdir(\"/Users/marie-louisekorte/Documents/Uni Leipzig/Lisbon/Data.nosync/\")\n",
    "#os.chdir(\"/Volumes/Jamie/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab5f5ae-7771-4b0b-ac1c-1e91745372a1",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3000d513-28ae-4dc9-8263-3e07bd3b20ea",
   "metadata": {},
   "source": [
    "### ERA5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9532cdd-0229-4d6c-9dc7-3379e27e9209",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load ERA5 mean turbulent surface stress\n",
    "# load data in chunks to avoid jupyter lab crashing\n",
    "MTSS_19th = xr.merge([xr.open_dataset(f) for f in glob.glob('ERA5/Surface_stress/Turbulent_mean/*_19*.nc')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330df4fb-3788-46c5-88f1-bc32346b44db",
   "metadata": {},
   "outputs": [],
   "source": [
    "MTSS_20th_N = xr.merge([xr.open_dataset(f) for f in glob.glob('ERA5/Surface_stress/Turbulent_mean/N_20*.nc')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b79bde7-8165-4934-82d9-d90165ea5932",
   "metadata": {},
   "outputs": [],
   "source": [
    "MTSS_20th_E = xr.merge([xr.open_dataset(f) for f in glob.glob('ERA5/Surface_stress/Turbulent_mean/E_20*.nc')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70f6018-6617-4812-aeb7-9e1b2aa80132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets  (and drop empty coordinate \"realization\")\n",
    "MTSS = xr.merge([MTSS_19th, MTSS_20th_N, MTSS_20th_E])\n",
    "MTSS = MTSS.drop_vars([\"realization\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0219494-b838-4a61-aae9-e44c897c9c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as netcdf \n",
    "MTSS.to_netcdf(\"MTSS.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2729e7f-817b-434f-99fd-e4a573c41977",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load ERA5 land sea mask\n",
    "LSM = xr.open_dataset('Land_sea_mask.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2815617-207d-44ab-b235-71793f92a3c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### UI SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af566f8-8563-4ad0-b0c9-7c7482505851",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load UI SST\n",
    "UI_SST = xr.open_dataset('UI_SST_CoastNET.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6740bc-3e9e-4461-946f-e8b5f2c579a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ECCO2 SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "540cbac9-72fb-4052-9eb2-30fce73cced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load ECCO2 SST\n",
    "for year in np.arange(1992, 2024):\n",
    "    for month in np.arange(1, 13):\n",
    "        ds = xr.open_dataset(f'ECCO2/ECCO2_SST/SST.1440x720.{year}{month:02d}.nc')\n",
    "        ds = ds.sel(LATITUDE_T = slice(35, 45), LONGITUDE_T = slice(340, 355))\n",
    "        if ((year == 1992) and (month == 1)):\n",
    "            ECCO2_SST = ds\n",
    "        ECCO2_SST = xr.merge([ECCO2_SST, ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6174f53-2e8d-4679-ae70-f6a1c5139a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load ECCO2 SST\n",
    "for year in np.arange(2020, 2024):\n",
    "    for month in np.arange(1, 13):\n",
    "        ds = xr.open_dataset(f'ECCO2/ECCO2_SSH/SSH.1440x720.{year}{month:02d}.nc')\n",
    "        ds = ds.sel(LATITUDE_T = slice(35, 45), LONGITUDE_T = slice(340, 355))\n",
    "        if ((year == 1992) and (month == 1)):\n",
    "            ECCO2_SSH = ds\n",
    "        ECCO2_SSH = xr.merge([ECCO2_SSH, ds])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac7c315-1127-422d-8526-4e6bd366f24f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### IBI SSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "83c0e76c-2908-4a20-92b2-95cd7bd42537",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load SSH\n",
    "SSH = xr.open_dataset('SSH_daily.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e77f4b-e40f-405c-8f52-0cb5ba120abc",
   "metadata": {},
   "source": [
    "### CoRTAD SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c8f29a0-a9d9-49bd-806d-f54c5b1546c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SST = xr.open_dataset('cortadv6_FilledSST.nc')\n",
    "SST = SST.drop_dims([\"nv\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234cff84-f44e-4b3d-8bf6-46a159036df6",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81a21399-40a4-495c-8acd-f57d9bdbdcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set wd to where I want the datasets to be saved\n",
    "os.chdir(\"/Users/marie-louisekorte/Documents/Uni Leipzig/Lisbon/Data.nosync/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc82f1a-19d1-499b-ac74-5f6990a0c801",
   "metadata": {},
   "source": [
    "### ERA5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e29a764-1d4a-4a12-91d5-4d059f45df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## process ERA5 - add land sea mask\n",
    "# add land-sea mask to the MTSS dataset\n",
    "MTSS['lsm'] = LSM.lsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32444386-e252-4893-aeb9-549585550601",
   "metadata": {},
   "outputs": [],
   "source": [
    "## process ERA5 - weekly resmaple\n",
    "# resample ERA5 data to same weekly resolution as SST upwelling index (Weekly mean Sat-Fr & time stamp Tue)\n",
    "# time = 'W-SAT' -> resamples to weekly time res. starting on a Saturday (default is Sunday)\n",
    "# closed = 'left' -> means [start date, end_date) i.e. start date is included and end_date is exluded in the interval I choose \n",
    "# label = 'left' -> the time stamp from the start of the interval is assigned\n",
    "MTSS_weekly_mean = MTSS.resample(time = 'W-SAT', closed = 'left', label = 'left').mean() \n",
    "# change time label ->I want to add 3 days to my time coordinate to move my time stamp from Sat to Tue to match SST upwelling index format\n",
    "MTSS_weekly_mean['time'] = MTSS_weekly_mean.time + np.timedelta64(3, 'D')\n",
    "\n",
    "# save as netcd\n",
    "MTSS_weekly_mean.to_netcdf(\"MTSS_weekly.nc\")\n",
    "\n",
    "# same for std\n",
    "#MTSS_weekly_std =  MTSS.resample(time = 'W-SAT', closed = 'left', label = 'left').std() \n",
    "#MTSS_weekly_std['time'] = MTSS_weekly_std.time + np.timedelta64(3, 'D')\n",
    "#MTSS_weekly_std.to_netcdf(\"MTSS_weekly_std.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d87cd72-80c3-4e0a-b232-4bb11b474a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## process ERA5 - calculate Ekman upwelling index\n",
    "# calculate upwelling index from wind stress dataset -> use my upwelling function (from my_functions.py)\n",
    "# UI_Ek = my_functions.calc_upwelling_index(MTSS, MTSS.lat, MTSS.lon, MTSS.metss, MTSS.mntss)\n",
    "\n",
    "# save as netcdf\n",
    "# UI_Ek.to_netcdf(\"UI_Ek.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d66f60-dd67-40d5-ac62-b41af0ddb722",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### SST UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6f9c97d-78f3-4bc0-9782-98d5a4f3e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## process SST UI - convert Kelvin to °C\n",
    "# also upedate the attributes\n",
    "UI_SST['Tmid'] = UI_SST.Tmid - 273.15\n",
    "UI_SST.Tmid.attrs.update({\"name\" : \"sea_surface_skin_temperature\", \"units\" : \"degree Celsius °C\"})\n",
    "UI_SST['Toff15W'] = UI_SST.Toff15W - 273.15\n",
    "UI_SST.Toff15W.attrs.update({\"name\" : \"sea_surface_skin_temperature\", \"units\" : \"degree Celsius °C\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a57e5348-f3d2-44d4-ba05-60ae97830f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## process SST UI - change sign of index\n",
    "UI_SST['UI'] = UI_SST.UI * -1\n",
    "UI_SST.UI.attrs.update({\"name\" : \"difference in sea_surface_skin_temperature\", \"units\" : \" degree Celsius °C\", \"method\" : \" Toff15 - Tmid\", \"info\" : \" > 2°C upwelling event\"})\n",
    "\n",
    "# save as netcdf\n",
    "UI_SST.to_netcdf(\"UI_SST.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f4f40b-a5ba-43ac-bceb-e673020f3959",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ECCO2 SST & SSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a69aac50-75a1-449f-81e2-a97b665b80c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## process rename lat, lon and time\n",
    "ECCO2 = xr.merge([ECCO2_SST, ECCO2_SSH])\n",
    "ECCO2 = ECCO2.rename({'LATITUDE_T' : 'lat', 'LONGITUDE_T' : 'lon', 'TIME' : 'time'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bf5b532-2570-4ce1-9d6f-71a683943149",
   "metadata": {},
   "outputs": [],
   "source": [
    "## process ECCO2 SST -> resample to weeky res\n",
    "ECCO2_weekly_mean = ECCO2.resample(time = 'W-SAT', closed = 'left', label = 'left').mean() \n",
    "ECCO2_weekly_mean['time'] = ECCO2_weekly_mean.time + np.timedelta64(3, 'D')\n",
    "\n",
    "# save as netcd\n",
    "ECCO2_weekly_mean.to_netcdf(\"ECCO2_weekly.nc\")\n",
    "\n",
    "## same for std\n",
    "#ECCO2_weekly_std =  ECCO2.resample(time = 'W-SAT', closed = 'left', label = 'left').std() \n",
    "#ECCO2_weekly_std['time'] = ECCO2_weekly_std.time + np.timedelta64(3, 'D')\n",
    "#ECCO2_weekly_std.to_netcdf(\"SSH_weekly_std.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6de53c-e645-4f9b-acfd-d3788cd3830f",
   "metadata": {},
   "source": [
    "### IBI SSH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "06869e58-cd4b-42bc-b94f-003f5bf7a220",
   "metadata": {},
   "outputs": [],
   "source": [
    "## process SSH -> resample to weeky res\n",
    "SSH_weekly_mean = SSH.resample(time = 'W-SAT', closed = 'left', label = 'left').mean() \n",
    "SSH_weekly_mean['time'] = SSH_weekly_mean.time + np.timedelta64(3, 'D')\n",
    "\n",
    "# save as netcd\n",
    "SSH_weekly_mean.to_netcdf(\"SSH_weekly.nc\")\n",
    "\n",
    "## same for std\n",
    "#SSH_weekly_std =  SSH.resample(time = 'W-SAT', closed = 'left', label = 'left').std() \n",
    "#SSH_weekly_std['time'] = SSH_weekly_std.time + np.timedelta64(3, 'D')\n",
    "#SSH_weekly_std.to_netcdf(\"SSH_weekly_std.nc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0796a10e-8104-4d29-87a0-cc8ed3be4bdf",
   "metadata": {},
   "source": [
    "### CoRTAD SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ca64a44-7ac3-4b7a-9e86-45e061c17c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## process CoRTAD SST \n",
    "# reverse order of lat\n",
    "SST = SST.reindex(lat=list(reversed(SST.lat)))\n",
    "\n",
    "# select research area\n",
    "SST = SST.sel(lat = slice(45, 35), lon = slice(-20, -5))\n",
    "\n",
    "SST['SST'] = SST.FilledSST - 273.15\n",
    "SST.SST.attrs.update({\"standard_name\" : \"sea_surface_skin_temperature\", \"units\" : \" degree Celsius °C\", \"info\" : \"WeeklySST - 273.15\"})\n",
    "\n",
    "# save as netcdf\n",
    "SST.to_netcdf(\"CoRTAD_weekly.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a14461a-33a8-4cdf-bd97-6aacd5a37b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "SST.to_netcdf(\"CoRTAD_weekly.nc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IbUp Py3.9.12",
   "language": "python",
   "name": "ibuppy3.9.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
